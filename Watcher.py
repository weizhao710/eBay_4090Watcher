import os
import time
import random
import json
import re
import requests
from bs4 import BeautifulSoup
import telebot

# ========== ç¯å¢ƒå˜é‡ ==========
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")
EBAY_URL = os.getenv("EBAY_URL")  # ä¾‹å¦‚ï¼šhttps://www.ebay.co.uk/sch/i.html?_nkw=4090&_sacat=27386&LH_PrefLoc=1&_sop=10&rt=nc

if not TELEGRAM_TOKEN or not CHAT_ID or not EBAY_URL:
    raise RuntimeError(
        "è¯·åœ¨ Railway Variables ä¸­è®¾ç½® TELEGRAM_TOKEN / CHAT_ID / EBAY_URL"
    )

# å¦‚æœ CHAT_ID æ˜¯çº¯æ•°å­—ï¼Œè½¬æˆ intï¼ˆTelegram åº“æ›´ç¨³ï¼‰
try:
    CHAT_ID = int(CHAT_ID)
except ValueError:
    # æœ‰çš„äººä¼šç”¨ @usernameï¼Œé‚£å°±ä¿æŒå­—ç¬¦ä¸²
    pass

bot = telebot.TeleBot(TELEGRAM_TOKEN)

SEEN_FILE = "seen_ids.json"

# æ´¾ç”Ÿå‡º RSS URLï¼ˆåŒä¸€ä¸ªæœç´¢ï¼ŒåŠ ä¸Š &_rss=1ï¼‰
if "_rss=1" in EBAY_URL:
    EBAY_RSS_URL = EBAY_URL
else:
    join_char = "&" if "?" in EBAY_URL else "?"
    EBAY_RSS_URL = EBAY_URL + f"{join_char}_rss=1"


# ========== å·¥å…·å‡½æ•° ==========

def load_seen_ids():
    """ä»æœ¬åœ° JSON è¯»å·²è§è¿‡çš„ item id é›†åˆ"""
    if not os.path.exists(SEEN_FILE):
        return set()
    try:
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return set(data)
    except Exception:
        return set()


def save_seen_ids(seen_ids):
    """ä¿å­˜å·²è§è¿‡çš„ item id é›†åˆåˆ°æœ¬åœ° JSON"""
    try:
        with open(SEEN_FILE, "w", encoding="utf-8") as f:
            json.dump(list(seen_ids), f)
    except Exception as e:
        print(f"ä¿å­˜ seen_ids å¤±è´¥: {e}")


def send_message(text: str):
    """å‘é€ Telegram æ¶ˆæ¯"""
    try:
        bot.send_message(CHAT_ID, text, disable_web_page_preview=False)
    except Exception as e:
        print(f"Telegram å‘é€å¤±è´¥: {e}")


def parse_price(text: str):
    """ä»ä»·æ ¼å­—ç¬¦ä¸²é‡ŒæŠ½ä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œæ¯”å¦‚ 'Â£1,299.99' -> 1299.99"""
    if not text:
        return None
    txt = text.replace(",", "").replace("\xa0", " ")
    m = re.search(r"(\d+(\.\d+)?)", txt)
    if not m:
        return None
    try:
        return float(m.group(1))
    except ValueError:
        return None


def extract_item_id_from_url(url: str):
    """ä» eBay é“¾æ¥ä¸­æŠ½ item id"""
    if not url:
        return None
    # å¸¸è§å½¢å¼ï¼š/itm/123456789012
    m = re.search(r"/itm/(\d+)", url)
    if m:
        return m.group(1)
    # å¤‡ç”¨ï¼šitem123456789012
    m = re.search(r"item(\d+)", url)
    if m:
        return m.group(1)
    # å¤‡ç”¨ï¼š/?hash=item123456789012
    m = re.search(r"(\d{10,})", url)
    if m:
        return m.group(1)
    return None


# ========== æŠ“ç½‘é¡µå‰ 3 æ¡ ==========

def fetch_html_top3():
    """
    ä»æ­£å¸¸æœç´¢é¡µ (EBAY_URL) æŠ“å–â€œæœ‰æ•ˆçš„å‰ä¸‰ä¸ªç»“æœâ€
    è¿”å›åˆ—è¡¨ [{
        id, title, price, url
    }]
    """
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0 Safari/537.36"
        )
    }

    resp = requests.get(EBAY_URL, headers=headers, timeout=15)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "html.parser")

    items = []
    for card in soup.select(".s-item"):
        if len(items) >= 3:
            break

        title_tag = card.select_one(".s-item__title")
        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        lower_title = title.lower()

        # è·³è¿‡å¹¿å‘Š/æç¤ºç±»ç»“æœ
        if any(
            kw in lower_title
            for kw in ["sponsored", "shop on ebay", "results matching fewer words"]
        ):
            continue

        # åªè¦æ ‡é¢˜é‡ŒåŒ…å« 4090ï¼ˆä½ åªç›‘æ§ 4090ï¼‰
        if "4090" not in lower_title:
            continue

        a = card.select_one(".s-item__link")
        if not a or not a.get("href"):
            continue
        url = a["href"]

        item_id = extract_item_id_from_url(url)
        if not item_id:
            continue

        price_tag = card.select_one(".s-item__price")
        price_text = price_tag.get_text(strip=True) if price_tag else None
        price = parse_price(price_text)

        clean_url = url.split("?_")[0]

        items.append(
            {
                "id": item_id,
                "title": title,
                "price": price,
                "url": clean_url,
                "source": "html",
            }
        )

    return items


# ========== æŠ“ RSS ==========

def fetch_rss_items():
    """
    ä» RSS (EBAY_RSS_URL) æŠ“å–è‹¥å¹²ç»“æœ
    è¿”å›åˆ—è¡¨ [{
        id, title, price(None), url
    }]
    """
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0 Safari/537.36"
        )
    }

    resp = requests.get(EBAY_RSS_URL, headers=headers, timeout=15)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text, "xml")

    items = []
    for item in soup.find_all("item"):
        title_tag = item.find("title")
        link_tag = item.find("link")

        if not title_tag or not link_tag:
            continue

        title = title_tag.get_text(strip=True)
        link = link_tag.get_text(strip=True)
        lower_title = title.lower()

        # åŒæ ·åªæ”¶ 4090
        if "4090" not in lower_title:
            continue

        item_id = extract_item_id_from_url(link)
        if not item_id:
            continue

        clean_url = link.split("?_")[0]

        items.append(
            {
                "id": item_id,
                "title": title,
                "price": None,  # RSS é‡Œä¸€èˆ¬ä¸ç›´æ¥ç»™ä»·æ ¼ï¼Œè¿™é‡Œå°±ä¸ç®¡äº†
                "url": clean_url,
                "source": "rss",
            }
        )

    return items


# ========== ä¸»æµç¨‹ ==========

def main():
    # éšæœºå»¶è¿Ÿ 0â€“5 ç§’ï¼Œé…åˆ Railway cronï¼ˆæ¯”å¦‚æ¯ 20 ç§’ä¸€æ¬¡ï¼‰
    delay = random.randint(0, 5)
    print(f"æœ¬æ¬¡å»¶è¿Ÿ {delay} ç§’åå¼€å§‹æŠ“å–")
    time.sleep(delay)

    seen_ids = load_seen_ids()
    print(f"å·²è®°å½• {len(seen_ids)} æ¡å†å² item")

    all_items = {}

    # 1) ç½‘é¡µå‰ 3 æ¡
    try:
        html_items = fetch_html_top3()
        print(f"HTML æŠ“åˆ° {len(html_items)} æ¡")
        for it in html_items:
            all_items[it["id"]] = it
    except Exception as e:
        print(f"æŠ“å– HTML å‡ºé”™: {e}")
        send_message(f"[eBay 4090 Watcher] æŠ“å– HTML å‡ºé”™ï¼š{e}")

    # 2) RSS
    try:
        rss_items = fetch_rss_items()
        print(f"RSS æŠ“åˆ° {len(rss_items)} æ¡")
        for it in rss_items:
            # å¦‚æœ HTML å·²æœ‰åŒ idï¼Œå°±ä¿ç•™ HTMLï¼ˆå› ä¸ºæœ‰ä»·æ ¼ï¼‰
            if it["id"] not in all_items:
                all_items[it["id"]] = it
    except Exception as e:
        print(f"æŠ“å– RSS å‡ºé”™: {e}")
        send_message(f"[eBay 4090 Watcher] æŠ“å– RSS å‡ºé”™ï¼š{e}")

    if not all_items:
        print("æœ¬æ¬¡æŠ“å–æ²¡æœ‰ä»»ä½•ç»“æœï¼ˆå¯èƒ½æ˜¯ç½‘ç»œ/ç»“æ„é—®é¢˜ï¼‰")
        return

    # åªå¯¹â€œä¹‹å‰æœªè§è¿‡â€çš„ id å‘é€šçŸ¥
    new_items = [it for it in all_items.values() if it["id"] not in seen_ids]

    if not new_items:
        print("æ²¡æœ‰æ–°çš„ item")
        return

    # æ›´æ–°å·²è§ ID
    for it in new_items:
        seen_ids.add(it["id"])
    save_seen_ids(seen_ids)

    # æŒ‰æ¥æºç®€å•æ’åºï¼šå…ˆ HTMLï¼ˆå› ä¸ºæ›´ç¨³å®šï¼‰ã€å† RSS
    new_items.sort(key=lambda x: x["source"])

    # æ¨é€
    for it in new_items:
        lines = [
            "ğŸ†• æ–° 4090 Listing",
            f"æ¥æºï¼š{ 'ç½‘é¡µå‰ 3 æ¡' if it['source']=='html' else 'RSS' }",
            f"æ ‡é¢˜ï¼š{it['title']}",
        ]
        if it["price"] is not None:
            lines.append(f"ä»·æ ¼ï¼šÂ£{it['price']}")
        lines.append(f"é“¾æ¥ï¼š{it['url']}")
        msg = "\n".join(lines)
        send_message(msg)
        print(f"å·²æ¨é€ï¼š{it['id']} - {it['title']} ({it['source']})")


if __name__ == "__main__":
    main()
