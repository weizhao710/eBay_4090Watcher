import os
import time
import random
import json
import re
import requests
from bs4 import BeautifulSoup
import telebot

# ========== ç¯å¢ƒå˜é‡ ==========
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")
EBAY_URL = (os.getenv("EBAY_URL") or "").strip()

if not TELEGRAM_TOKEN or not CHAT_ID or not EBAY_URL:
    raise RuntimeError(
        "è¯·åœ¨ Railway Variables ä¸­è®¾ç½® TELEGRAM_TOKEN / CHAT_ID / EBAY_URL"
    )

# CHAT_ID å°è¯•è½¬æ•°å­—ï¼ˆæ›´ç¨³å®šï¼Œä¸æ˜¯æ•°å­—ä¹Ÿæ²¡å…³ç³»ï¼‰
try:
    CHAT_ID = int(CHAT_ID)
except:
    pass

bot = telebot.TeleBot(TELEGRAM_TOKEN)

SEEN_FILE = "seen_ids.json"

# è‡ªåŠ¨ç”Ÿæˆ RSS é“¾æ¥
if "_rss=1" in EBAY_URL:
    EBAY_RSS_URL = EBAY_URL
else:
    join_char = "&" if "?" in EBAY_URL else "?"
    EBAY_RSS_URL = EBAY_URL + f"{join_char}_rss=1"


# ========== å·¥å…·å‡½æ•° ==========
def load_seen_ids():
    if not os.path.exists(SEEN_FILE):
        return set()
    try:
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    except:
        return set()


def save_seen_ids(seen_ids):
    try:
        with open(SEEN_FILE, "w", encoding="utf-8") as f:
            json.dump(list(seen_ids), f)
    except:
        pass


def send_message(text: str):
    try:
        bot.send_message(CHAT_ID, text, disable_web_page_preview=False)
    except Exception as e:
        print(f"Telegram å‘é€å¤±è´¥: {e}")


def parse_price(text: str):
    if not text:
        return None
    txt = text.replace(",", "").replace("\xa0", " ")
    m = re.search(r"(\d+(\.\d+)?)", txt)
    if not m:
        return None
    try:
        return float(m.group(1))
    except:
        return None


def extract_item_id_from_url(url: str):
    if not url:
        return None
    m = re.search(r"/itm/(\d+)", url)
    if m:
        return m.group(1)
    m = re.search(r"item(\d+)", url)
    if m:
        return m.group(1)
    m = re.search(r"(\d{10,})", url)
    if m:
        return m.group(1)
    return None


# ========== æŠ“ç½‘é¡µå‰ä¸‰æ¡ ==========
def fetch_html_top3():
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0 Safari/537.36"
        )
    }

    resp = requests.get(EBAY_URL, headers=headers, timeout=25)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    items = []
    for card in soup.select(".s-item"):
        if len(items) >= 3:
            break

        title_tag = card.select_one(".s-item__title")
        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        lower_title = title.lower()

        # è·³å¹¿å‘Š
        if any(kw in lower_title for kw in ["sponsored", "shop on ebay", "results matching"]):
            continue

        # å¿…é¡»åŒ…å« 4090
        if "4090" not in lower_title:
            continue

        a = card.select_one(".s-item__link")
        if not a or not a.get("href"):
            continue

        url = a["href"]
        item_id = extract_item_id_from_url(url)
        if not item_id:
            continue

        price_tag = card.select_one(".s-item__price")
        price = parse_price(price_tag.get_text(strip=True)) if price_tag else None

        clean_url = url.split("?_")[0]

        items.append({
            "id": item_id,
            "title": title,
            "price": price,
            "url": clean_url,
            "source": "html",
        })

    return items


# ========== æŠ“ RSS ==========
def fetch_rss_items():
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0 Safari/537.36"
        )
    }

    resp = requests.get(EBAY_RSS_URL, headers=headers, timeout=25)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "xml")

    items = []
    for item in soup.find_all("item"):
        title = item.find("title").get_text(strip=True)
        link = item.find("link").get_text(strip=True)

        if "4090" not in title.lower():
            continue

        item_id = extract_item_id_from_url(link)
        if not item_id:
            continue

        clean_url = link.split("?_")[0]

        items.append({
            "id": item_id,
            "title": title,
            "price": None,
            "url": clean_url,
            "source": "rss",
        })

    return items


# ========== è·‘ä¸€è½® ==========
def run_once():
    seen_ids = load_seen_ids()
    print(f"å·²è®°å½• {len(seen_ids)} æ¡å†å² item")

    all_items = {}

    # HTML
    try:
        html_items = fetch_html_top3()
        print(f"HTML æŠ“åˆ° {len(html_items)} æ¡")
        for it in html_items:
            all_items[it["id"]] = it
    except Exception as e:
        print(f"æŠ“å– HTML å‡ºé”™: {e}")

    # RSS
    try:
        rss_items = fetch_rss_items()
        print(f"RSS æŠ“åˆ° {len(rss_items)} æ¡")
        for it in rss_items:
            if it["id"] not in all_items:
                all_items[it["id"]] = it
    except Exception as e:
        print(f"æŠ“å– RSS å‡ºé”™: {e}")

    if not all_items:
        print("æœ¬æ¬¡æŠ“å–æ²¡æœ‰ä»»ä½•ç»“æœ")
        return

    # æ‰¾æ–° id
    new_items = [it for it in all_items.values() if it["id"] not in seen_ids]

    if not new_items:
        print("æ²¡æœ‰æ–°çš„ item")
        return

    # è®°å½• seen
    for it in new_items:
        seen_ids.add(it["id"])
    save_seen_ids(seen_ids)

    # æŒ‰ HTML > RSS æ’åº
    new_items.sort(key=lambda x: x["source"])

    # æ¨é€
    for it in new_items:
        lines = [
            "ğŸ†• æ–° 4090 Listing",
            f"æ¥æºï¼š{'ç½‘é¡µå‰ 3 æ¡' if it['source']=='html' else 'RSS'}",
            f"æ ‡é¢˜ï¼š{it['title']}"
        ]
        if it["price"]:
            lines.append(f"ä»·æ ¼ï¼šÂ£{it['price']}")
        lines.append(f"é“¾æ¥ï¼š{it['url']}")

        send_message("\n".join(lines))
        print(f"å·²æ¨é€ï¼š{it['id']} - {it['title']} ({it['source']})")


# ========== è‡ªå¾ªç¯ ==========
if __name__ == "__main__":
    while True:
        try:
            print("====== å¼€å§‹æ–°ä¸€è½®æŠ“å– ======")
            run_once()
        except Exception as e:
            print(f"ä¸»å¾ªç¯å‡ºé”™ï¼š{e}")

        sleep_time = 20 + random.randint(0, 5)
        print(f"æœ¬è½®æŠ“å–ç»“æŸï¼Œä¼‘æ¯ {sleep_time} ç§’...\n")
        time.sleep(sleep_time)
